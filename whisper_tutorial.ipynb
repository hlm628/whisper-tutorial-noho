{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4323031a-0afc-4445-9e9a-e8b3186250ef",
   "metadata": {},
   "source": [
    "# Whisper Tutorial \n",
    "\n",
    "[Whisper](https://openai.com/research/whisper) is a speech recognition model, first released by OpenAI in September 2022. It uses a transformer-based, encoder-decoder architecture (paper [here](https://cdn.openai.com/papers/whisper.pdf)), and was trained using 680,000 hours of audio from a range of sources. Along with transcription, it can translate from over 50 languages to English - including M훮ori.\n",
    "\n",
    "It is also available in a variety of model 'sizes', from `tiny` (39 million parameters) to `large` (1.5 billion). This allows the user to trade inference speed for accuracy, depending on available hardware and desired application. GPUs are required to run the `large` model efficiently.\n",
    "\n",
    "Despite the name, OpenAI has been reticent to release model weights for most of their other models (GPT-$x$, DALL-E etc.), preferring to serve them from behind their APIs; Whisper is an exception."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3009121d-1416-4439-aebb-9621569b7217",
   "metadata": {},
   "source": [
    "## Transcription using local model\n",
    "\n",
    "Using Whisper is surprisingly easy. This tutorial will show you a couple of ways to do so.\n",
    "\n",
    "First, we must install the necessary Python libraries. If you are running this notebook inside the Docker image provided, they will already be installed. If not, I have provided a `requirements.txt` file which will help you out. You will also need to install [FFmpeg](https://ffmpeg.org/). \n",
    "\n",
    "To install from the requirements file using pip, uncomment the shell command below and run it (this will take a while if Pytorch needs to be installed):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be2080f-fa94-4bc2-a42c-623451d45eaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c7eede-02ab-49d8-a71b-1986f8a09d0b",
   "metadata": {},
   "source": [
    "We will start by testing transcription with a simple audio clip. Here it is, with playback handily embedded in our notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d538833-9b6f-49cb-992d-0b890f7bbc2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "Audio('data/example.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2099a671-524e-432e-a34c-a2701c48fdc8",
   "metadata": {},
   "source": [
    "Now run the following commands to load the `tiny` version of Whisper (you are welcome to try the `large` version if you like, but be warned it is 2.7GB and very slow on a plain old CPU!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520db53b-7ccd-441e-8699-c7750d465ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "tiny_model = whisper.load_model('tiny')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35df41a0-60f9-4287-9fb2-61e773ecd25e",
   "metadata": {},
   "source": [
    "To transcribe our example WAV file, simply run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5552de-cbc7-4099-992d-b0e7d99c4f9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tiny_model.transcribe('data/example.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f558210-b9d0-4b20-a233-11ee439f3acb",
   "metadata": {},
   "source": [
    "As you will see, even the `tiny` model takes a while to run. In fact, let's time it. The result will vary depending on your machine. Note also that I am suppressing the annoying warning message by supplying the argument `fp16=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5248e272-984d-41a8-a5d3-f5aec51c8695",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "result=tiny_model.transcribe('data/example.wav', fp16=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2565a-7f96-428d-a16b-f07151ed4b67",
   "metadata": {},
   "source": [
    "For comparison's sake, let's try the second smallest model (appropriately called `small`)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a902ce4-ab72-4970-be36-175c39334b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "small_model=whisper.load_model('small')\n",
    "small_model.transcribe('data/example.wav', fp16=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044ecad1-9cdf-4d52-a3a9-e7d1235d7a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "result=small_model.transcribe('data/example.wav', fp16=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4191f4d-9bef-4443-a616-2f255dabfa47",
   "metadata": {},
   "source": [
    "Not very promising. Thankfully for those of us without expensive hardware, OpenAI have recently begun to offer Whisper as part of their managed API service. This (allegedly) hosts version 2 of their `large` model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb3aaa3-cb5a-487e-a5bb-4d4282e91f59",
   "metadata": {},
   "source": [
    "## Transcription using API\n",
    "\n",
    "In order to use the Whisper API (and other OpenAI products), you will need to create an account. More instructions can be found here: [https://platform.openai.com/signup/api-key](https://platform.openai.com/signup/api-key)\n",
    "\n",
    "Once you done that, generate an [API secret key](https://platform.openai.com/account/api-keys), copy it to the `credentials.template` file, save as `credentials` (note: this will be ignored by git!) and run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb13df12-2be2-468a-bcbb-7bffb58a1735",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('credentials', 'r') as f:\n",
    "    secret_key = f.read()\n",
    "    \n",
    "import openai\n",
    "openai.api_key = secret_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74baf53a-18f4-4ba3-824a-0d1df7a4a26e",
   "metadata": {},
   "source": [
    "Transcribing via API in Python is as simple as this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e658022-4ad8-473a-89d0-99a9767a62b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_file = open(\"data/example.wav\", \"rb\")\n",
    "transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8fb370-1a40-43e0-bd40-7691bb1dd506",
   "metadata": {},
   "source": [
    "Note the result is the same as above. Before you think \"this isn't worth it\", let's move on to \"real world data\" that will make you appreciate the benefits of the `large` model..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a668164-e9ba-4c4b-978f-d5ba66b5c702",
   "metadata": {},
   "source": [
    "## Downloading New Zealand English dataset (RNZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ee8672-417b-40ae-97e5-e2b2b654ce54",
   "metadata": {},
   "source": [
    "A recent project I worked on involved building a tool for transcribing [New Zealand English](https://en.wikipedia.org/wiki/New_Zealand_English). Speech recognition models have traditionally struggled with this dialect - compared to, say, General American English - not to mention the various borrowed words from te reo M훮ori (\"kia ora\", \"wh훮nau\" etc.) and place names.\n",
    "\n",
    "To get a feel for how well Whisper does, I have prepared some clips from RNZ's programme Morning Report. Each clip can be downloaded in MP3 format from the supplied URL, and the content between the specified start/end timestamps is transcribed (by a human) in the `Text` field.\n",
    "\n",
    "Let's have a look at this data, using Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7990c605-1e78-4410-a29f-8f264a1e9179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rnz_metadata = pd.read_csv('data/morning_report.csv')\n",
    "rnz_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aa424f-6a62-41a8-b210-3b86c47ef06f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rnz_metadata.loc[1,'Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe865729-abb6-40b7-8c9b-7152fdb11272",
   "metadata": {},
   "source": [
    "We can use the `wget` and `os` Python packages to download these MP3s directly and save them in a directory called `data/rnz/raw` (not tracked by git). Here is an example, using one of the rows above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3af028-4ad0-4717-8bf1-70ae5072763f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "import os\n",
    "rnz_data_dir = 'data/rnz'\n",
    "rnz_raw_dir = os.path.join(rnz_data_dir, 'raw')\n",
    "os.makedirs(rnz_raw_dir, exist_ok=True)\n",
    "filepath=wget.download(rnz_metadata.loc[1,'URL'],out=rnz_raw_dir)\n",
    "Audio(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612b3355-9a25-4521-8b43-4d91caf9b533",
   "metadata": {},
   "source": [
    "If we want to extract only the transcribed segment, we can use the `pydub` library. We will use a helper function to extract raw millisecond timings from the timestamps supplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8f77e8-9ff0-4719-9a6d-28be594cd14a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper function\n",
    "def timestamp_to_ms(timestamp):\n",
    "    m, s = timestamp.split(':')\n",
    "    s, ss = s.split('.')\n",
    "    m, s, ss = [int(t) for t in [m,s,ss]]\n",
    "    return m*60*1000+s*1000+ss\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import re\n",
    "\n",
    "clip_filepath = re.sub('/raw/','/clips/',filepath)\n",
    "rnz_clips_dir = os.path.join(rnz_data_dir, 'clips')\n",
    "os.makedirs(rnz_clips_dir, exist_ok=True)\n",
    "\n",
    "start_ms = timestamp_to_ms(rnz_metadata.loc[1,'Start'])\n",
    "end_ms = timestamp_to_ms(rnz_metadata.loc[1,'End'])\n",
    "\n",
    "sound = AudioSegment.from_mp3(filepath)\n",
    "sound[start_ms:end_ms]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e8b42f-0803-44e7-a870-a31b8567bfe1",
   "metadata": {},
   "source": [
    "We will save this trimmed clip in the directory `data/rnz/clips`. Let's see how our `tiny` model handles it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9951e3c9-73e9-44d1-8dc0-1f649579d73b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sound[start_ms:end_ms].export(clip_filepath, format=\"mp3\")\n",
    "\n",
    "tiny_model.transcribe(clip_filepath, fp16=False)['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53e48c2-ac02-44dc-91fc-8a584529ad0a",
   "metadata": {},
   "source": [
    "Well, that could have been better! It seems Whisper is confused about which language is being spoken, and has \"split the difference\" in some way. We can give it a hint by specifying that the language is English (`en`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ff59b0-ba5b-43ea-9a8c-b225bc57f9e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tiny_model.transcribe(clip_filepath, fp16=False, language='en')['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d67545c-5c97-4205-a063-78193e962d37",
   "metadata": {},
   "source": [
    "This is better, but some words are still causing issues. Let's try the API/`large` model for comparison. We will write a handy function to make these calls easier to construct going forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89592dc1-4b06-43ab-b861-2c8d948571fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transcribe_api(mp3_path):\n",
    "    with open(mp3_path, \"rb\") as audiofile:\n",
    "        transcript=openai.Audio.transcribe(\"whisper-1\", audiofile, response_format='text', language='en')\n",
    "    return transcript.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb57cf5-cf6d-4ecc-bcab-fcf4a9f0c58a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcribe_api(clip_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db4d120-7f83-40fc-b8e5-6d418c105d58",
   "metadata": {},
   "source": [
    "Much better! Now let's run these models over the entire dataset. I will be interested to see what you find among the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b596bd1f-dd44-4227-9a1c-875b1a1f6846",
   "metadata": {},
   "source": [
    "## Whisper accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6dd90-0e9d-4983-926a-1d4bbf917815",
   "metadata": {},
   "source": [
    "First, we will consolidate some of the steps above into a handy function that takes the supplied URL and start/end times, and downloads the MP3, trims the relevant selection and saves it in the right place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad2f207-459d-449e-803b-c35f553744cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mp3_download_trim(row, raw_dir=rnz_raw_dir,clips_dir=rnz_clips_dir):\n",
    "    basename=row.URL.split('/')[-1]\n",
    "    raw_filepath=os.path.join(raw_dir, basename)\n",
    "    # insure against doing this many times!\n",
    "    if not os.path.exists(raw_filepath):\n",
    "        wget.download(row.URL,out=raw_dir)\n",
    "    \n",
    "    # include start/end in filepath name\n",
    "    clip_filepath=os.path.join(clips_dir, f\"{row.Start}_{row.End}_basename\")\n",
    "    if not os.path.exists(clip_filepath):\n",
    "        start_ms = timestamp_to_ms(row.Start)\n",
    "        end_ms = timestamp_to_ms(row.End)\n",
    "        sound = AudioSegment.from_mp3(filepath)\n",
    "        sound[start_ms:end_ms].export(clip_filepath)\n",
    "    return clip_filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2d52a4-1517-492a-8a50-eeeebb42a846",
   "metadata": {},
   "source": [
    "We can then exploit the Pandas `apply` method to run this across our entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29537d2e-2b86-4dce-89bb-2ed2de017cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rnz_metadata['Filepath']=rnz_metadata.apply(mp3_download_trim,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b64c502-75ee-4df7-8b7c-8868c3094205",
   "metadata": {},
   "source": [
    "The filepath to the relevant clip is now a new field in the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218e3421-b812-4b9b-8641-17f2775302bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rnz_metadata['Filepath'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bece1aa2-a1e4-448b-b8f4-b9091d5a9ddf",
   "metadata": {},
   "source": [
    "Now let's use a similar `apply` call to transcribe the relevant clip, using our local/`tiny` model. Be patient - this could take a while. I have used a sample here to save testing time, but feel free to vary `N` below to suit your device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e987228c-0733-49a6-833a-bb9c40d63be2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set seed\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "N=3\n",
    "\n",
    "rnz_metadata=rnz_metadata.sample(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602cb3b2-4d17-47e0-a6da-355b3b30f881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rnz_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d0c00-7d08-4826-addf-b036a4d0457f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rnz_metadata['Tiny transcription'] = rnz_metadata.Filepath.apply(lambda x: tiny_model.transcribe(x, fp16=False, language='en')['text'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbed1e1b-668e-4462-b393-2f15880d0bb9",
   "metadata": {},
   "source": [
    "And the same with the API/`large` model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88d45db-710d-4e4c-b3d1-d30b13d21196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rnz_metadata['API transcription'] = rnz_metadata.Filepath.apply(transcribe_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f00895f-64e0-43e1-b673-1e81651643ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rnz_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229dd0db-7536-4790-8f2d-582aabc80388",
   "metadata": {},
   "source": [
    "Let's pick a random example and see how the transcriptions compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de9ba67-9c27-4512-9eca-5b2e47399553",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example = rnz_metadata.sample(1)\n",
    "Audio(example.Filepath.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3588d2da-2cfe-4d73-b812-402233b48e77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example.Text.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8772feb-25b2-48f7-9fef-088d27edd29d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example['Tiny transcription'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56753417-6eae-4d70-a364-b5d878637aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example['API transcription'].tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebaf7c9-ddc4-4377-9dfc-fef1ef5a041d",
   "metadata": {},
   "source": [
    "For a more comprehensive comparison of accuracy, we need metrics. A common one for comparing transcriptions is the [Word Error Rate](https://en.wikipedia.org/wiki/Word_error_rate), or WER. This scores a transcription on how many substitutions, insertions and deletions of words are needed to recreate the ground truth transcription from the inferred one.\n",
    "\n",
    "We can implement this in Python using the `jiwer` package. Note that, in order to be statistically fair, we may want to first \"normalise\" both texts to remove unimportant differences such as how numbers are rendered from influencing the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316682cc-c689-4d51-80cc-28f43b73e876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "\n",
    "# a single example\n",
    "wer(example['Tiny transcription'].tolist()[0], example.Text.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7c332f-9ac1-4ab7-a762-4590229342b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# over entire dataset\n",
    "tiny_wer = wer(rnz_metadata['Tiny transcription'].tolist(), rnz_metadata.Text.tolist())\n",
    "api_wer = wer(rnz_metadata['API transcription'].tolist(), rnz_metadata.Text.tolist())\n",
    "print(f\"WER (Tiny model): {tiny_wer}, WER (API model): {api_wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ae51c3-a0ac-4606-a37a-87d09bf9cc76",
   "metadata": {},
   "source": [
    "## Ethical considerations\n",
    "\n",
    "After seeing that Whisper can (at least attempt to) transcribe and translate te reo M훮ori, you may be wondering: where/how did it learn to do that?\n",
    "\n",
    "The paper claims that training data originates from [Fleurs](https://huggingface.co/datasets/google/fleurs), a multilingual collection of recordings of a common set of phrases, translated into 102 languages. The method Google use to select and approach the various speakers involved is somewhat opaque.\n",
    "\n",
    "This raises many interesting ethical questions - several of which have probably already occurred to you!\n",
    "\n",
    "Te Hiku Media have written an article which discusses these issues quite comprehensively, and which I highly recommend reading: [https://blog.papareo.nz/whisper-is-another-case-study-in-colonisation/](https://blog.papareo.nz/whisper-is-another-case-study-in-colonisation/) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
